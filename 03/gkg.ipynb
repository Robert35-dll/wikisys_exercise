{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ Challenge #A1: Eigene Knowledge Base\n",
    "\n",
    "Als Basis f√ºr meinen Knowledge Graph habe ich die NieR (Computerspiel-) Serie genommen und die wesentlichste Entit√§ten mit ihren Beziehungen gezeichnet:\n",
    "\n",
    "![](./nier_knowledge_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ Challenge #A2: OpenAI GPT <3 Google Knowledge Graph\n",
    "\n",
    "Manche (wenn nicht alle) APIs fordert einen API-Schl√ºssel, um damit interagieren zu k√∂nnen. Diese werden in der Regel in einer separaten Datei gespeichert, die zur Laufzeit wie folgt gelesen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Set up your API keys\n",
    "GOOGLE_KG_API_KEY = 'your_google_kg_api_key'\n",
    "OPENAI_API_KEY = 'your_openai_api_key'\n",
    "\n",
    "# Keeping secrets secret („ÄÇ„Éªœâ„Éª„ÄÇ)\n",
    "with open('./secrets.json', 'r') as file:\n",
    "    keys = json.load(file)\n",
    "    GOOGLE_KG_API_KEY = keys['GOOGLE']\n",
    "    OPENAI_API_KEY = keys['OPENAI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um mit dem Thema fortzufahren, definiere ich eine Anfrage zu meiner ausgew√§hlten NieR Serie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"NieR:Automata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Knowledge Graph\n",
    "\n",
    "Um Anfragen an Google KG API stellen zu k√∂nnen, kann die Request-URL verwendet werden:\n",
    "```\n",
    "https://kgsearch.googleapis.com/v1/entities:search?query=YOUR_QUERY&key=API_KEY&limit=1&indent=True\n",
    "```\n",
    "\n",
    "**Wichtig:** man muss die Knowledge Graph Dienst explizit f√ºr den Projekt aktivieren, dem der API-Schl√ºssel zugewiesen ist. Wenn nicht der Fall, alle Anfragen geben den Hinweis mit dem Link zum Portal zur√ºck.\n",
    "\n",
    "Den URL kann man mit Python `requests` Bibliothek ein bisschen sch√∂ner bauen und f√ºr die Anfrage verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# The function to query Google Knowledge Graph\n",
    "def query_knowledge_graph(query):\n",
    "    url = \"https://kgsearch.googleapis.com/v1/entities:search\"\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': 1,\n",
    "        'indent': True,\n",
    "        'key': GOOGLE_KG_API_KEY\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    if 'itemListElement' in data and len(data['itemListElement']) > 0:\n",
    "        entity = data['itemListElement'][0]['result']\n",
    "        return entity.get('name', 'Unknown'), entity.get('description', 'No description available.')\n",
    "    else:\n",
    "        return \"Entity not found\", \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI GPT\n",
    "\n",
    "OpenAI bietet einen sehr umfangreichen API f√ºr seine eigene Modelle. F√ºr kurze Chit-Chat Abfragen ist ein [`Completion`](https://github.com/openai/openai-python/blob/main/src/openai/resources/completions.py) Objekt von [`Responses API`](https://platform.openai.com/docs/api-reference/responses) ausreichend. Um die Ausgabe flexibler definieren zu k√∂nnen, kann man auch `ChatCompletion` mit angepassten Eingaben verwenden. Dieses repr√§sentiert eine Antwort eines LLM auf konkrete Anfrage als Chatnachricht und wir folgenderma√üen erstellt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Define the function to use GPT-3 for generating responses\n",
    "def generate_gpt_response(question):\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    print(f'[?] {question}')\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[{ \"role\": \"system\", \"content\": \"Answer in two short sentences.\" },\n",
    "                  { \"role\": \"user\", \"content\": question }\n",
    "                  ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Wissenskapazit√§ten von LLMs erweitern zu k√∂nnen, werden die beiden APIs in ein Pipeline gebunden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[?] What can you tell me about NieR: Automata? Video game\n",
      " |-< Knowledge Graph Entity: NieR: Automata\n",
      " |-< Knowledge Graph Info: Video game\n",
      "[>] LLM Response: NieR: Automata is an action role-playing game developed by PlatinumGames and published by Square Enix, released in 2017. It features fast-paced combat, a deep storyline exploring themes of consciousness and humanity, and multiple endings.\n"
     ]
    }
   ],
   "source": [
    "# Main function to integrate the LLM and Knowledge Graph\n",
    "def knowledge_graph_llm_pipeline(query):\n",
    "    # Step 1: Query the graph's entities for the specified query\n",
    "    entity_name, entity_description = query_knowledge_graph(query)\n",
    "    # Step 2: Generate a question for GPT-3 using the knowledge graph info\n",
    "    if entity_name != \"Entity not found\":\n",
    "        gpt_question = f\"What can you tell me about {entity_name}? {entity_description}\"\n",
    "    else:\n",
    "        gpt_question = f\"Tell me more about {query} in general.\"\n",
    "\n",
    "    # Step 3: Get a response from the LLM\n",
    "    gpt_response = generate_gpt_response(gpt_question)\n",
    "    \n",
    "    return {\n",
    "        'Entity': entity_name,\n",
    "        'Knowledge Graph Info': entity_description,\n",
    "        'LLM Response': gpt_response\n",
    "    }\n",
    "\n",
    "result = knowledge_graph_llm_pipeline(query)\n",
    "print(\" |-< Knowledge Graph Entity:\", result['Entity'])\n",
    "print(\" |-< Knowledge Graph Info:\", result['Knowledge Graph Info'])\n",
    "print(\"[>] LLM Response:\", result['LLM Response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikisy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
