{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/htw-logo.png\" width=150>\n",
    "\n",
    "\n",
    "**I758 Wissens- und KI-basierte Systeme**\n",
    "\n",
    "# Lineare Regression mit Kreuzvalidierung\n",
    "Quelle: Kaggle  / Anpassungen CK\n",
    " \n",
    "\n",
    "<font color=\"green\"><b>KLAUSURTAUGLICH.</b></font>\n",
    "Dieses Notebook gehört zu den fünf Notebooks, die Sie für die Klausur einreichen können. Bei vollständiger und korrekter Bearbeitung **erhalten Sie Punkte für die Abgabe, die zu Ihrer Klausur addiert werden.**\n",
    "\n",
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "\n",
    "Auch dieses Notebook enthält wieder eine Reihe von neuen Bibliotheken. Legen Sie eine neue virtuelle Umgebung an oder erweitern Sie die bestehende `ml-class`. Studieren Sie dazu die Import-Anweisungen.\n",
    "\n",
    "\n",
    "Tipp: Vermutlich wird Ihr Leben einfacher, wenn Sie eine `requirements.txt` anlegen. Mehr dazu finden Sie z.B. in [diesem Post von Stackoverflow](https://stackoverflow.com/questions/66899666/how-to-install-from-requirements-txt) \n",
    "\n",
    "\n",
    "\n",
    "### Cross-Validation with Linear Regression\n",
    "\n",
    "This notebook demonstrates how to do feature selection and cross-validation (CV) with linear regression as an example (it is heavily used in almost all modelling techniques such as decision trees, SVM etc.). We will mainly use `sklearn` to do cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "\n",
    "Let us first load and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import warnings # supress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import Housing.csv\n",
    "housing = pd.read_csv('data/Housing.csv', sep=\";\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# number of samples \n",
    "len(housing.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first experiment, we'll do regression with only one feature, `area`, and we'll build a model to predict the price. Let's filter the data so it only contains `area` and `price`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "Reduzieren Sie den Data Frame.\n",
    "</div>\n",
    "\n",
    "Nutzen Sie die Pandas-Funktion `loc[ , ]`, um `housing` auf die Spalten `area` und `price` zu reduzieren. Speichern Sie das Ergebnis in `df`. Der Befehl `df.head()` sollte im Anschluss einen Output wie im Bild liefern.\n",
    "\n",
    "<img src=\"./img/image_df_head.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter only area and price\n",
    "df = # your code here\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn offers a wide variety of usefull *transformer* functions to scale, normalize and otherwise manipulate features.\n",
    "\n",
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "\n",
    "Skalieren Sie den Data Frame.\n",
    "</div>\n",
    "\n",
    "Nutzen Sie einen geeigneeten `Scaler`, um die beiden Spalten von `df` so zu skalieren, dass die Werte zwischen 0 und 1 liegen. Speichern Sie das Ergebnis in `df`. Der Befehl `df.head()` sollte im Anschluss einen Output wie im Bild liefern.\n",
    "\n",
    "<img src=\"./img/image_df_scaler.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recaling the variables (both)\n",
    "df_columns = # your code here\n",
    "\n",
    "scaler = # your code here\n",
    "df = # your code here\n",
    "\n",
    "# rename columns (since now its an np array)\n",
    "df = pd.DataFrame(df)\n",
    "df.columns = df_columns\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data to see the relationship between our single input feauture and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise area-price relationship\n",
    "sns.regplot(x=\"area\", y=\"price\", data=df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "Machen Sie sich klar, was Sie im Plot sehen. Was bedeuten die Werte an den Achsen? Was bedeutet die Verteilung der Punkte entlang x und y? Sieht man einen Zusammenhang zwischen x und y?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df, \n",
    "                                     train_size = 0.7, \n",
    "                                     test_size = 0.3, \n",
    "                                     random_state = 10)\n",
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments to Understand Overfitting\n",
    "\n",
    "In this section, let's quickly go through some experiments to understand what overfitting looks like. We'll run some experiments using polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y for both train and test sets\n",
    "# reshaping is required since sklearn requires the data to be in shape\n",
    "# (n, 1), not as a series of shape (n, )\n",
    "X_train = df_train['area']\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "y_train = df_train['price']\n",
    "\n",
    "X_test = df_test['area']\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y_test = df_test['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "You already know simple linear regression:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_1$\n",
    "\n",
    "In polynomial regression of degree $n$, we fit a curve of the form:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_1 + \\beta_2x_1^2 + \\beta_3x_1^3 ... + \\beta_nx_1^n$\n",
    "\n",
    "In the experiment below, we have fitted polynomials of various degrees on the housing data and compared their performance on train and test sets.\n",
    "\n",
    "In sklearn, polynomial features can be generated using the `PolynomialFeatures` class. Also, to perform `LinearRegression` and `PolynomialFeatures` in tandem, we will use the module `sklearn_pipeline` - it basically creates the features and feeds the output to the model (in that sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now predict the y labels (for both train and test sets) and store the predictions in a table. Each row of the table is one data point, each column is a value of $n$ (degree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>   </th>\n",
    "    <th>degree-1</th>\n",
    "    <th>degree-2</th> \n",
    "    <th>degree-3</th>\n",
    "    <th>...</th>\n",
    "    <th>degree-n</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>x1</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>x2</th>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <th>x3</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <th>...</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <th>xn</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit multiple polynomial features\n",
    "degrees = [1, 2, 3, 6, 10, 20]\n",
    "\n",
    "# initialise y_train_pred and y_test_pred matrices to store the train and test predictions\n",
    "# each row is a data point, each column a prediction using a polynomial of some degree\n",
    "y_train_pred = np.zeros((len(X_train), len(degrees)))\n",
    "y_test_pred = np.zeros((len(X_test), len(degrees)))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    \n",
    "    # make pipeline: create features, then feed them to linear_reg model\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict on test and train data\n",
    "    # store the predictions of each degree in the corresponding column\n",
    "    y_train_pred[:, i] = model.predict(X_train)\n",
    "    y_test_pred[:, i] = model.predict(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise train and test predictions\n",
    "# note that the y axis is on a log scale\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# train data\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.yscale('log')\n",
    "plt.title(\"Train data\")\n",
    "for i, degree in enumerate(degrees):    \n",
    "    plt.scatter(X_train, y_train_pred[:, i], s=15, label=str(degree))\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "# test data\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.yscale('log')\n",
    "plt.title(\"Test data\")\n",
    "for i, degree in enumerate(degrees):    \n",
    "    plt.scatter(X_test, y_test_pred[:, i], label=str(degree))\n",
    "    plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "Machen Sie sich klar, was Sie in den Plots sehen. Notieren Sie im Feld unten Ihre Beobachtungen.\n",
    "<ul>\n",
    "<li>In welchem Zusammenhang stehen die bunten Linien zu den blauen Punkten?</li>\n",
    "<li>Was bedeuten die verschiedenen Farben der Linien?</li>\n",
    "<li>Können Sie aus den Linien (im Feld \"Test\") irgendetwas sehen zu der Frage, welches Modell das \"Beste\" sein könnte?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ihre Antworten hier ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate the quality of our predictions by comparing predictions to true values. We use the R2 statistic for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare r2 for train and test sets (for all polynomial fits)\n",
    "print(\"R-squared values: \\n\")\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    train_r2 = round(sklearn.metrics.r2_score(y_train, y_train_pred[:, i]), 2)\n",
    "    test_r2 = round(sklearn.metrics.r2_score(y_test, y_test_pred[:, i]), 2)\n",
    "    print(\"Polynomial degree {0}: train score={1}, test score={2}\".format(degree, \n",
    "                                                                         train_r2, \n",
    "                                                                         test_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "\n",
    "Schauen Sie sich die Zahlen mit den R squared values in Ruhe an. Bemerken Sie das absurde Verhalten?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Building a Model Without Cross-Validation\n",
    "\n",
    "Let's now build a multiple regression model. First, let's build a vanilla MLR model without any cross-validation etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "\n",
    "# list of all the \"yes-no\" binary categorical variables\n",
    "# we'll map yes to 1 and no to 0\n",
    "binary_vars_list =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
    "\n",
    "# defining the map function\n",
    "def binary_map(x):\n",
    "    return x.map({'yes': 1, \"no\": 0})\n",
    "\n",
    "# applying the function to the housing variables list\n",
    "housing[binary_vars_list] = housing[binary_vars_list].apply(binary_map)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dummy' variables\n",
    "# get dummy variables for 'furnishingstatus' \n",
    "# also, drop the first column of the resulting df (since n-1 dummy vars suffice)\n",
    "status = pd.get_dummies(housing['furnishingstatus'], drop_first = True)\n",
    "status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat the dummy variable df with the main df\n",
    "housing = pd.concat([housing, status], axis = 1)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'furnishingstatus' since we alreday have the dummy vars\n",
    "housing.drop(['furnishingstatus'], axis = 1, inplace = True)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Into Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "\n",
    "Das können Sie schon: erstellen Sie aus `housing` Test- und Trainingsdaten (70% Train, 30% Test).  Skalieren Sie anschließend alle numerischen Werte in Train mit einem `MinMaxScaler`!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train-test 70-30 split\n",
    "df_train, df_test = # Ihr code hier\n",
    "\n",
    "# rescale the features\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# apply scaler() to all the numeric columns \n",
    "numeric_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']\n",
    "df_train[numeric_vars] = # Ihr code hier\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "\n",
    "Das können Sie auch schon: Übertragen Sie nun die Daten auf das Test-Set!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply rescaling to the test set also\n",
    "df_test[numeric_vars] = # Ihr Code hier\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide into X_train, y_train, X_test, y_test\n",
    "y_train = df_train.pop('price')\n",
    "X_train = df_train\n",
    "\n",
    "y_test = df_test.pop('price')\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we haven't rescaled the test set yet, which we'll need to do later while making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using RFE \n",
    "\n",
    "Now, we have 13 predictor features. To build the model using Recursive Feature Elemination (RFE), we need to tell RFE how many features we want in the final model. It then runs a feature elimination algorithm. \n",
    "\n",
    "Note that the number of features to be used in the model is a **hyperparameter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of max features\n",
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first model with an arbitrary choice of n_features\n",
    "# running RFE with number of, let's say features=10\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "rfe = RFE(lm, n_features_to_select=10)             \n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuples of (feature name, whether selected, ranking)\n",
    "# note that the 'rank' is > 1 for non-selected features\n",
    "list(zip(X_train.columns,rfe.support_,rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict prices of X_test\n",
    "y_pred = rfe.predict(X_test)\n",
    "\n",
    "# evaluate the model on test set\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "\n",
    "Halten Sie kurz inne und überlegen Sie, was Sie hier sehen. Halten Sie unten fest, welches Subset die RFE jetzt eigentlich ausgewählt hat.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notieren Sie hier das Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try with another value of RFE\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "rfe = RFE(lm, n_features_to_select=6)             \n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# predict prices of X_test\n",
    "y_pred = rfe.predict(X_test)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Arbeitsauftrag ohne Dokumentation</b> \n",
    "Wir haben jetzt also eine RFE mit n=6 und eine RFE mit n=10 gemacht. Welche ist besser?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problems in the Current Approach\n",
    "\n",
    "In train-test split, we have three options:\n",
    "1. **Simply split into train and test**: But that way tuning a hyperparameter makes the model 'see' the test data (i.e. knowledge of test data leaks into the model)\n",
    "2. **Split into train, validation, test sets**: Then the validation data would eat into the training set\n",
    "3. **Cross-validation**: Split into train and test, and train multiple models by sampling the train set. Finally, just test once on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation: A Quick Recap\n",
    "\n",
    "The following figure illustrates k-fold cross-validation with k=4. There are some other schemes to divide the training set, we'll look at them briefly later.\n",
    "\n",
    "![](https://chkra.github.io/wikisys_lecture/workshops/08/08/img/kfold_cross.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation in sklearn\n",
    "\n",
    "Let's now experiment with k-fold CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# k-fold CV (using all the 13 variables)\n",
    "lm = LinearRegression()\n",
    "scores = cross_val_score(lm, X_train, y_train, scoring='r2', cv=5)\n",
    "scores      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the other way of doing the same thing (more explicit)\n",
    "\n",
    "# create a KFold object with 5 splits \n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "scores = cross_val_score(lm, X_train, y_train, scoring='r2', cv=folds)\n",
    "scores   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "\n",
    "Sie sollten 5 Werte in der Variable scores sehen. Überlegen Sie kurz: was bedeuten diese Werte?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tune other metrics, such as MSE\n",
    "scores = cross_val_score(lm, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hyperparameter Tuning Using Grid Search Cross-Validation\n",
    "\n",
    "A common use of cross-validation is for tuning hyperparameters of a model. The most common technique is what is called **grid search** cross-validation.\n",
    "![](https://chkra.github.io/wikisys_lecture/workshops/08/08/img/hyperparam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features in X_train\n",
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step-1: create a cross-validation scheme\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "\n",
    "# step-2: specify range of hyperparameters to tune\n",
    "hyper_params = [{'n_features_to_select': list(range(1, len(X_train.columns)))}]\n",
    "\n",
    "\n",
    "# step-3: perform grid search\n",
    "# 3.1 specify model\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "rfe = RFE(lm)             \n",
    "\n",
    "# 3.2 call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = rfe, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(X_train, y_train)                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting cv results\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "\n",
    "Schauen Sie auf die Tabelle. Was ist das beste Ergebnis und in welcher Spalte / welcher Zeile sehen Sie das?\n",
    "</div>\n",
    "\n",
    "PS: Vielleicht hilft [dieser Link](https://stackoverflow.com/questions/54608088/what-is-gridsearch-cv-results-could-any-explain-all-the-things-in-that-i-e-me) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can choose the optimal value of number of features and build a final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#FF5F00\"><b>AUFGABE:</b></span><br>\n",
    "\n",
    "\n",
    "Nutzen Sie den Wert aus der letzten Frage und korrigieren Sie bei Bedarf den Wert der Variable n_features_optimal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model\n",
    "n_features_optimal = 18\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "rfe = RFE(lm, n_features_to_select=n_features_optimal)             \n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# predict prices of X_test\n",
    "y_pred = lm.predict(X_test)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test score is very close to the 'mean test score' on the k-folds (about 60%). In general, the mean score estimated by CV will usually be a good estimate of the test score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Types of Cross-Validation Schemes\n",
    "\n",
    "\n",
    "1. **K-Fold** cross-validation: Most common\n",
    "2. **Leave One Out (LOO)**: Takes each data point as the 'test sample' once, and trains the model on the rest n-1 data points. Thus, it trains n total models.\n",
    "    - Advantage: Utilises the data well since each model is trained on n-1 samples\n",
    "    - Disadvantage: Computationally expensive\n",
    "3. **Leave P-Out (LPO)**: Creat all possible splits after leaving p samples out. For n data points, there are (nCp) possibile train-test splits.\n",
    "4. (**For classification problems**) **Stratified K-Fold**: Ensures that the relative class proportion is approximately preserved in each train and validation fold. Important when ther eis huge class imbalance (e.g. 98% good customers, 2% bad).\n",
    "\n",
    "#### Additional Reading ####\n",
    "The sklearn documentation enlists all CV schemes <a href=\"http://scikit-learn.org/stable/modules/cross_validation.html\">here.</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
